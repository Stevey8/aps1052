{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook cannot run yet: it will run when Keras catches up with TensorFlow version 2.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "import numpy as np\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from matplotlib import pyplot\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import pearsonr\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy spearman: -0.49999999999999994\n",
      "tf spearman -0.5\n",
      "SpearmanrResult(correlation=-0.49999999999999994, pvalue=0.39100221895577053)\n"
     ]
    }
   ],
   "source": [
    "def spearman_correlation(predictions, targets):\n",
    "#From:https: //github.com/numerai/example-scripts/blob/master/example_model.py#L21\n",
    "\n",
    "    if not isinstance(predictions, pd.Series):\n",
    "        predictions = pd.Series(predictions)\n",
    "        \n",
    "    ranked_preds = predictions.rank(pct = True, method = \"first\")\n",
    "    return np.corrcoef(ranked_preds, targets)[0, 1]\n",
    "\n",
    "def corrcoef(x, y):\n",
    "#np.corrcoef() implemented with tf primitives\n",
    "\n",
    "    mx = tf.math.reduce_mean(x)\n",
    "    my = tf.math.reduce_mean(y)\n",
    "    xm, ym = x - mx, y - my\n",
    "    r_num = tf.math.reduce_sum(xm * ym)\n",
    "    r_den = tf.norm(xm) * tf.norm(ym)\n",
    "    return r_num / (r_den + tf.keras.backend.epsilon())\n",
    "\n",
    "def tf_spearman_correlation(predictions, targets):\n",
    "    ranked_preds = tf.cast(tf.argsort(tf.argsort(predictions, stable = True)), targets.dtype)\n",
    "    return corrcoef(ranked_preds, targets)\n",
    "\n",
    "targets = np.array([0.0, 0.25, 0.5, 0.75, 1.0], dtype = np.float32)\n",
    "predictions = np.random.rand(targets.shape[0])\n",
    "\n",
    "print(\"numpy spearman:\", spearman_correlation(predictions, targets))\n",
    "result = tf_spearman_correlation(tf.convert_to_tensor(predictions, dtype=tf.float32), tf.convert_to_tensor(targets, dtype=tf.float32))\n",
    "with tf.Session() as sess:\n",
    "    scalar = result.eval()\n",
    "\n",
    "print(\"tf spearman\", scalar)\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "print (spearmanr(targets,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spearman_loss(y_true: tf.Tensor,\n",
    "                                 y_pred: tf.Tensor) -> float:\n",
    "    \"\"\"Spearman correlation coefficient\"\"\"\n",
    "\n",
    "    x = y_true\n",
    "    y = y_pred\n",
    "    y = tf.cast(tf.argsort(tf.argsort(y, stable = True)), targets.dtype) #argsort is not a differentiable operation\n",
    "    xm, ym = x - K.mean(x), y - K.mean(y)\n",
    "    r_num = K.sum(tf.multiply(xm, ym))\n",
    "    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))\n",
    "    r = r_num / (r_den + K.epsilon())\n",
    "    r = K.maximum(K.minimum(r, 1.0), -1.0)\n",
    "\n",
    "    return 1 - K.square(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate regression dataset\n",
    "X = np.genfromtxt(\"X.txt\", delimiter=\",\")  \n",
    "y = np.genfromtxt(\"y.txt\", delimiter=\",\")  \n",
    "\n",
    "# split into train and test\n",
    "n_train = 500\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "\n",
    "#scale the y data\n",
    "m = np.mean(trainy)\n",
    "s = np.std(trainy)\n",
    "trainy = (trainy-m)/s\n",
    "testy = (testy-m)/s\n",
    "\n",
    "m = np.mean(trainX)\n",
    "s = np.std(trainX)\n",
    "trainX = (trainX-m)/s\n",
    "testX = (testX-m)/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 500 samples, validate on 500 samples\n",
      "Epoch 1/100\n",
      "500/500 [==============================] - 0s 231us/step - loss: 1.1712 - val_loss: 0.4017\n",
      "Epoch 2/100\n",
      "500/500 [==============================] - 0s 34us/step - loss: 0.2517 - val_loss: 0.1899\n",
      "Epoch 3/100\n",
      "500/500 [==============================] - 0s 35us/step - loss: 0.1374 - val_loss: 0.1359\n",
      "Epoch 4/100\n",
      "500/500 [==============================] - 0s 37us/step - loss: 0.0850 - val_loss: 0.0974\n",
      "Epoch 5/100\n",
      "500/500 [==============================] - 0s 37us/step - loss: 0.0664 - val_loss: 0.0829\n",
      "Epoch 6/100\n",
      "500/500 [==============================] - 0s 36us/step - loss: 0.0550 - val_loss: 0.0735\n",
      "Epoch 7/100\n",
      "500/500 [==============================] - 0s 34us/step - loss: 0.0474 - val_loss: 0.0661\n",
      "Epoch 8/100\n",
      "500/500 [==============================] - 0s 34us/step - loss: 0.0446 - val_loss: 0.0659\n",
      "Epoch 9/100\n",
      "500/500 [==============================] - 0s 35us/step - loss: 0.0416 - val_loss: 0.0576\n",
      "Epoch 10/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.0337 - val_loss: 0.0525\n",
      "Epoch 11/100\n",
      "500/500 [==============================] - 0s 35us/step - loss: 0.0307 - val_loss: 0.0493\n",
      "Epoch 12/100\n",
      "500/500 [==============================] - 0s 36us/step - loss: 0.0274 - val_loss: 0.0455\n",
      "Epoch 13/100\n",
      "500/500 [==============================] - 0s 34us/step - loss: 0.0255 - val_loss: 0.0442\n",
      "Epoch 14/100\n",
      "500/500 [==============================] - 0s 36us/step - loss: 0.0240 - val_loss: 0.0403\n",
      "Epoch 15/100\n",
      "500/500 [==============================] - 0s 33us/step - loss: 0.0232 - val_loss: 0.0398\n",
      "Epoch 16/100\n",
      "500/500 [==============================] - 0s 34us/step - loss: 0.0223 - val_loss: 0.0376\n",
      "Epoch 17/100\n",
      "500/500 [==============================] - 0s 36us/step - loss: 0.0215 - val_loss: 0.0362\n",
      "Epoch 18/100\n",
      "500/500 [==============================] - 0s 36us/step - loss: 0.0193 - val_loss: 0.0343\n",
      "Epoch 19/100\n",
      "500/500 [==============================] - 0s 34us/step - loss: 0.0200 - val_loss: 0.0325\n",
      "Epoch 20/100\n",
      "500/500 [==============================] - 0s 34us/step - loss: 0.0183 - val_loss: 0.0341\n",
      "Epoch 21/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.0169 - val_loss: 0.0293\n",
      "Epoch 22/100\n",
      "500/500 [==============================] - 0s 34us/step - loss: 0.0156 - val_loss: 0.0298\n",
      "Epoch 23/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.0145 - val_loss: 0.0278\n",
      "Epoch 24/100\n",
      "500/500 [==============================] - 0s 34us/step - loss: 0.0145 - val_loss: 0.0269\n",
      "Epoch 25/100\n",
      "500/500 [==============================] - 0s 38us/step - loss: 0.0135 - val_loss: 0.0263\n",
      "Epoch 26/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.0130 - val_loss: 0.0267\n",
      "Epoch 27/100\n",
      "500/500 [==============================] - 0s 34us/step - loss: 0.0127 - val_loss: 0.0242\n",
      "Epoch 28/100\n",
      "500/500 [==============================] - 0s 34us/step - loss: 0.0122 - val_loss: 0.0240\n",
      "Epoch 29/100\n",
      "500/500 [==============================] - ETA: 0s - loss: 0.010 - 0s 34us/step - loss: 0.0116 - val_loss: 0.0238\n",
      "Epoch 30/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.0117 - val_loss: 0.0232\n",
      "Epoch 31/100\n",
      "500/500 [==============================] - 0s 34us/step - loss: 0.0109 - val_loss: 0.0223\n",
      "Epoch 32/100\n",
      "500/500 [==============================] - 0s 34us/step - loss: 0.0107 - val_loss: 0.0217\n",
      "Epoch 33/100\n",
      "500/500 [==============================] - 0s 34us/step - loss: 0.0108 - val_loss: 0.0217\n",
      "Epoch 34/100\n",
      "500/500 [==============================] - 0s 36us/step - loss: 0.0102 - val_loss: 0.0205\n",
      "Epoch 35/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.0109 - val_loss: 0.0213\n",
      "Epoch 36/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.0095 - val_loss: 0.0198\n",
      "Epoch 37/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.0094 - val_loss: 0.0197\n",
      "Epoch 38/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.0094 - val_loss: 0.0195\n",
      "Epoch 39/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.0091 - val_loss: 0.0186\n",
      "Epoch 40/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.0083 - val_loss: 0.0186\n",
      "Epoch 41/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.0083 - val_loss: 0.0177\n",
      "Epoch 42/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.0078 - val_loss: 0.0179\n",
      "Epoch 43/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.0079 - val_loss: 0.0174\n",
      "Epoch 44/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.0075 - val_loss: 0.0170\n",
      "Epoch 45/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.0072 - val_loss: 0.0177\n",
      "Epoch 46/100\n",
      "500/500 [==============================] - 0s 34us/step - loss: 0.0073 - val_loss: 0.0163\n",
      "Epoch 47/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.0069 - val_loss: 0.0172\n",
      "Epoch 48/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.0069 - val_loss: 0.0161\n",
      "Epoch 49/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.0066 - val_loss: 0.0163\n",
      "Epoch 50/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.0066 - val_loss: 0.0157\n",
      "Epoch 51/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.0065 - val_loss: 0.0156\n",
      "Epoch 52/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.0063 - val_loss: 0.0151\n",
      "Epoch 53/100\n",
      "500/500 [==============================] - 0s 30us/step - loss: 0.0063 - val_loss: 0.0159\n",
      "Epoch 54/100\n",
      "500/500 [==============================] - 0s 34us/step - loss: 0.0064 - val_loss: 0.0154\n",
      "Epoch 55/100\n",
      "500/500 [==============================] - 0s 34us/step - loss: 0.0059 - val_loss: 0.0145\n",
      "Epoch 56/100\n",
      "500/500 [==============================] - 0s 34us/step - loss: 0.0058 - val_loss: 0.0149\n",
      "Epoch 57/100\n",
      "500/500 [==============================] - 0s 30us/step - loss: 0.0059 - val_loss: 0.0145\n",
      "Epoch 58/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.0056 - val_loss: 0.0148\n",
      "Epoch 59/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.0056 - val_loss: 0.0142\n",
      "Epoch 60/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.0056 - val_loss: 0.0147\n",
      "Epoch 61/100\n",
      "500/500 [==============================] - 0s 30us/step - loss: 0.0058 - val_loss: 0.0142\n",
      "Epoch 62/100\n",
      "500/500 [==============================] - 0s 30us/step - loss: 0.0051 - val_loss: 0.0140\n",
      "Epoch 63/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.0052 - val_loss: 0.0153\n",
      "Epoch 64/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.0054 - val_loss: 0.0134\n",
      "Epoch 65/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.0052 - val_loss: 0.0135\n",
      "Epoch 66/100\n",
      "500/500 [==============================] - 0s 30us/step - loss: 0.0050 - val_loss: 0.0132\n",
      "Epoch 67/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.0049 - val_loss: 0.0135\n",
      "Epoch 68/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.0049 - val_loss: 0.0130\n",
      "Epoch 69/100\n",
      "500/500 [==============================] - 0s 30us/step - loss: 0.0050 - val_loss: 0.0131\n",
      "Epoch 70/100\n",
      "500/500 [==============================] - 0s 30us/step - loss: 0.0047 - val_loss: 0.0126\n",
      "Epoch 71/100\n",
      "500/500 [==============================] - 0s 30us/step - loss: 0.0048 - val_loss: 0.0125\n",
      "Epoch 72/100\n",
      "500/500 [==============================] - 0s 30us/step - loss: 0.0045 - val_loss: 0.0130\n",
      "Epoch 73/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.0050 - val_loss: 0.0127\n",
      "Epoch 74/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.0046 - val_loss: 0.0124\n",
      "Epoch 75/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.0043 - val_loss: 0.0124\n",
      "Epoch 76/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.0045 - val_loss: 0.0125\n",
      "Epoch 77/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.0044 - val_loss: 0.0121\n",
      "Epoch 78/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.0042 - val_loss: 0.0122\n",
      "Epoch 79/100\n",
      "500/500 [==============================] - 0s 30us/step - loss: 0.0043 - val_loss: 0.0119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.0042 - val_loss: 0.0119\n",
      "Epoch 81/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.0040 - val_loss: 0.0116\n",
      "Epoch 82/100\n",
      "500/500 [==============================] - 0s 30us/step - loss: 0.0039 - val_loss: 0.0120\n",
      "Epoch 83/100\n",
      "500/500 [==============================] - 0s 30us/step - loss: 0.0041 - val_loss: 0.0114\n",
      "Epoch 84/100\n",
      "500/500 [==============================] - 0s 30us/step - loss: 0.0039 - val_loss: 0.0118\n",
      "Epoch 85/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.0039 - val_loss: 0.0112\n",
      "Epoch 86/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.0038 - val_loss: 0.0114\n",
      "Epoch 87/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.0037 - val_loss: 0.0111\n",
      "Epoch 88/100\n",
      "500/500 [==============================] - 0s 30us/step - loss: 0.0038 - val_loss: 0.0112\n",
      "Epoch 89/100\n",
      "500/500 [==============================] - 0s 34us/step - loss: 0.0037 - val_loss: 0.0110\n",
      "Epoch 90/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.0037 - val_loss: 0.0111\n",
      "Epoch 91/100\n",
      "500/500 [==============================] - 0s 30us/step - loss: 0.0037 - val_loss: 0.0108\n",
      "Epoch 92/100\n",
      "500/500 [==============================] - 0s 30us/step - loss: 0.0035 - val_loss: 0.0109\n",
      "Epoch 93/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.0035 - val_loss: 0.0109\n",
      "Epoch 94/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.0035 - val_loss: 0.0110\n",
      "Epoch 95/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.0034 - val_loss: 0.0111\n",
      "Epoch 96/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.0035 - val_loss: 0.0114\n",
      "Epoch 97/100\n",
      "500/500 [==============================] - 0s 30us/step - loss: 0.0036 - val_loss: 0.0106\n",
      "Epoch 98/100\n",
      "500/500 [==============================] - 0s 32us/step - loss: 0.0033 - val_loss: 0.0105\n",
      "Epoch 99/100\n",
      "500/500 [==============================] - 0s 30us/step - loss: 0.0034 - val_loss: 0.0108\n",
      "Epoch 100/100\n",
      "500/500 [==============================] - 0s 30us/step - loss: 0.0033 - val_loss: 0.0105\n",
      "500/500 [==============================] - 0s 12us/step\n",
      "500/500 [==============================] - 0s 12us/step\n",
      "Train: 0.003, Test: 0.010\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# mlp with scaled outputs on the regression problem\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=20, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "# compile model using spearman_correlation los function\n",
    "#model.compile(loss=spearman_loss, optimizer=SGD(lr=0.01, momentum=0.9)) #no gradient\n",
    "model.compile(loss='mean_squared_error', optimizer=SGD(lr=0.01, momentum=0.9))\n",
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=100, verbose=1)\n",
    "# evaluate the model\n",
    "train_mse = model.evaluate(trainX, trainy, verbose=1)\n",
    "test_mse = model.evaluate(testX, testy, verbose=1)\n",
    "#print('Train: %.3f, Test: %.3f' % (train_mse[1], test_mse[1])) #when using custom loss and custom metric\n",
    "print('Train: %.3f, Test: %.3f' % (train_mse, test_mse)) \n",
    "#plot loss during training\n",
    "pyplot.title('Loss / Mean Squared Error')\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.py4u.net/discuss/199027"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 4         \n",
      "=================================================================\n",
      "Total params: 4\n",
      "Trainable params: 4\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "@tf.custom_gradient\n",
    "def py_loss_fn(y_true, y_pred):\n",
    "  \"\"\" This function takes eager tensors as inputs which can be explicitly\n",
    "  converted to np.arrays via EagerTensor.numpy() or implicitly converted\n",
    "  by applying numpy operations to them.\n",
    "\n",
    "  However, once tf operations are no longer used it means that the function has to\n",
    "  implement its own gradient function.\n",
    "  \"\"\"\n",
    "  def grad(dy):\n",
    "    \"\"\" Compute gradients for function inputs.\n",
    "        Ignore input[0] (y_true) since that is model.targets[0]\n",
    "    \"\"\"\n",
    "    g = np.mean(-dy * np.sign(y_true - y_pred), axis=1)[:, np.newaxis]\n",
    "    return None, g\n",
    "\n",
    "  return np.mean(np.abs(y_true - y_pred), axis=1), grad\n",
    "\n",
    "def eager_loss_fn(y_true, y_pred):\n",
    "  \"\"\" If tf operations are used on eager tensors auto diff works without issues\n",
    "  \"\"\"\n",
    "  return tf.reduce_mean(tf.abs(y_true - y_pred))\n",
    "\n",
    "def loss_fn(y_true, y_pred, **kw_args):\n",
    "  \"\"\" This function takes tensors as inputs. Numpy operations are not valid.\n",
    "  \"\"\"\n",
    "#   loss = tf.py_function(eager_loss_fn, inp=[y_true, y_pred], Tout=tf.float32)\n",
    "  \n",
    "  loss = tf.py_function(py_loss_fn, inp=[y_true, y_pred], Tout=tf.float32)\n",
    "\n",
    "  return loss\n",
    "\n",
    "def make_model():\n",
    "  \"\"\" Linear regression model with custom loss \"\"\"\n",
    "  inp = Input(shape=(4,))\n",
    "  out = Dense(1, use_bias=False)(inp)\n",
    "  model = Model(inp, out)\n",
    "  model.compile('adam', loss_fn)\n",
    "  return model\n",
    "\n",
    "model = make_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.674992084503174\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "FACTORS = np.arange(4) + 1\n",
    "def test_fn(x):\n",
    "  return np.dot(x, FACTORS.T)\n",
    "\n",
    "X = np.random.rand(3, 4)\n",
    "Y = np.apply_along_axis(test_fn, 1, X)\n",
    "\n",
    "history = model.fit(X, Y, epochs=1000, verbose=False)\n",
    "print(history.history['loss'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/aponte411/numerai_train/blob/master/metrics.py\n",
    "https://pretagteam.com/question/how-to-compute-spearman-correlation-in-tensorflow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
