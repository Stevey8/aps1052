{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "import numpy as np\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from matplotlib import pyplot\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import pearsonr\n",
    "import keras.backend as K\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy spearman: 0.09999999999999996\n",
      "tf spearman 0.1\n",
      "SpearmanrResult(correlation=0.09999999999999999, pvalue=0.8728885715695383)\n"
     ]
    }
   ],
   "source": [
    "def spearman_correlation(predictions, targets):\n",
    "#From:https: //github.com/numerai/example-scripts/blob/master/example_model.py#L21\n",
    "\n",
    "    if not isinstance(predictions, pd.Series):\n",
    "        predictions = pd.Series(predictions)\n",
    "        \n",
    "    ranked_preds = predictions.rank(pct = True, method = \"first\")\n",
    "    return np.corrcoef(ranked_preds, targets)[0, 1]\n",
    "\n",
    "def corrcoef(x, y):\n",
    "#np.corrcoef() implemented with tf primitives\n",
    "\n",
    "    mx = tf.math.reduce_mean(x)\n",
    "    my = tf.math.reduce_mean(y)\n",
    "    xm, ym = x - mx, y - my\n",
    "    r_num = tf.math.reduce_sum(xm * ym)\n",
    "    r_den = tf.norm(xm) * tf.norm(ym)\n",
    "    return r_num / (r_den + tf.keras.backend.epsilon())\n",
    "\n",
    "def tf_spearman_correlation(predictions, targets):\n",
    "    ranked_preds = tf.cast(tf.argsort(tf.argsort(predictions, stable = True)), targets.dtype)\n",
    "    return corrcoef(ranked_preds, targets)\n",
    "\n",
    "targets = np.array([0.0, 0.25, 0.5, 0.75, 1.0], dtype = np.float32)\n",
    "predictions = np.random.rand(targets.shape[0])\n",
    "\n",
    "print(\"numpy spearman:\", spearman_correlation(predictions, targets))\n",
    "result = tf_spearman_correlation(tf.convert_to_tensor(predictions, dtype=tf.float32), tf.convert_to_tensor(targets, dtype=tf.float32))\n",
    "with tf.Session() as sess:\n",
    "    scalar = result.eval()\n",
    "\n",
    "print(\"tf spearman\", scalar)\n",
    "\n",
    "print (spearmanr(targets,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spearman_loss(y_true, y_pred):\n",
    "#Generates an error do not use\n",
    "    \"\"\"Spearman correlation coefficient\"\"\"\n",
    "\n",
    "    x = y_true\n",
    "    y = y_pred\n",
    "    y = tf.cast(tf.argsort(tf.argsort(y, stable = True)), targets.dtype) #argsort is not a differentiable operation\n",
    "    xm, ym = x - K.mean(x), y - K.mean(y)\n",
    "    r_num = K.sum(tf.multiply(xm, ym))\n",
    "    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))\n",
    "    r = r_num / (r_den + K.epsilon())\n",
    "    r = K.maximum(K.minimum(r, 1.0), -1.0)\n",
    "\n",
    "    return  tf.constant(1.0, dtype=x.dtype) - K.square(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This spearman_metric using py_func works in my setup but if you have problems with it, \n",
    "just use the one underneath (commented out) because\n",
    "py_func doesn't always work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spearman_metric(y_true, y_pred):\n",
    "    \"\"\"Spearman correlation coefficient\"\"\"\n",
    "\n",
    "    r = tf.py_function(spearmanr, inp=[y_true, y_pred], Tout=tf.float32)\n",
    "    \n",
    "    return  1 - r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def spearman_metric(y_true, y_pred):\n",
    "    \"\"\"Spearman correlation coefficient\"\"\"\n",
    "\n",
    "    x = y_true\n",
    "    y = y_pred\n",
    "    y = tf.cast(tf.argsort(tf.argsort(y, stable = True)), targets.dtype)\n",
    "    xm, ym = x - K.mean(x), y - K.mean(y)\n",
    "    r_num = K.sum(tf.multiply(xm, ym))\n",
    "    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))\n",
    "    r = r_num / (r_den + K.epsilon())\n",
    "    r = K.maximum(K.minimum(r, 1.0), -1.0)\n",
    "\n",
    "    return  tf.constant(1.0, dtype=x.dtype) - K.square(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 20\n",
    "# generate regression dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=num_features, noise=0.1, random_state=1)\n",
    "\n",
    "# split into train and test\n",
    "n_train = 500\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "\n",
    "# reshape 1d arrays to 2d arrays\n",
    "trainy = trainy.reshape(len(trainy), 1)\n",
    "testy = testy.reshape(len(trainy), 1)\n",
    "\n",
    "# create scaler\n",
    "scaler = StandardScaler()\n",
    "# fit scaler on training dataset\n",
    "scaler.fit(trainy)\n",
    "# transform training dataset\n",
    "trainy = scaler.transform(trainy)\n",
    "# transform test dataset\n",
    "testy = scaler.transform(testy)\n",
    "\n",
    "# fit scaler on training dataset\n",
    "scaler.fit(trainX)\n",
    "# transform training dataset\n",
    "trainX = scaler.transform(trainX)\n",
    "# transform test dataset\n",
    "testX = scaler.transform(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 500 samples, validate on 500 samples\n",
      "Epoch 1/100\n",
      "500/500 [==============================] - 0s 321us/step - loss: 1.1138 - spearman_metric: 0.5615 - val_loss: 0.5485 - val_spearman_metric: 0.2211\n",
      "Epoch 2/100\n",
      "500/500 [==============================] - 0s 101us/step - loss: 0.2671 - spearman_metric: 0.1556 - val_loss: 0.2097 - val_spearman_metric: 0.0964\n",
      "Epoch 3/100\n",
      "500/500 [==============================] - 0s 98us/step - loss: 0.1371 - spearman_metric: 0.0978 - val_loss: 0.1528 - val_spearman_metric: 0.0737\n",
      "Epoch 4/100\n",
      "500/500 [==============================] - 0s 98us/step - loss: 0.0967 - spearman_metric: 0.0618 - val_loss: 0.1315 - val_spearman_metric: 0.0618\n",
      "Epoch 5/100\n",
      "500/500 [==============================] - 0s 103us/step - loss: 0.0749 - spearman_metric: 0.0477 - val_loss: 0.1108 - val_spearman_metric: 0.0546\n",
      "Epoch 6/100\n",
      "500/500 [==============================] - 0s 102us/step - loss: 0.0593 - spearman_metric: 0.0346 - val_loss: 0.0995 - val_spearman_metric: 0.0511\n",
      "Epoch 7/100\n",
      "500/500 [==============================] - 0s 100us/step - loss: 0.0497 - spearman_metric: 0.0359 - val_loss: 0.0888 - val_spearman_metric: 0.0473\n",
      "Epoch 8/100\n",
      "500/500 [==============================] - 0s 116us/step - loss: 0.0439 - spearman_metric: 0.0291 - val_loss: 0.0864 - val_spearman_metric: 0.0421\n",
      "Epoch 9/100\n",
      "500/500 [==============================] - 0s 126us/step - loss: 0.0425 - spearman_metric: 0.0289 - val_loss: 0.0777 - val_spearman_metric: 0.0406\n",
      "Epoch 10/100\n",
      "500/500 [==============================] - 0s 106us/step - loss: 0.0357 - spearman_metric: 0.0239 - val_loss: 0.0699 - val_spearman_metric: 0.0390\n",
      "Epoch 11/100\n",
      "500/500 [==============================] - 0s 124us/step - loss: 0.0320 - spearman_metric: 0.0278 - val_loss: 0.0673 - val_spearman_metric: 0.0357\n",
      "Epoch 12/100\n",
      "500/500 [==============================] - 0s 144us/step - loss: 0.0303 - spearman_metric: 0.0237 - val_loss: 0.0667 - val_spearman_metric: 0.0323\n",
      "Epoch 13/100\n",
      "500/500 [==============================] - 0s 116us/step - loss: 0.0290 - spearman_metric: 0.0231 - val_loss: 0.0620 - val_spearman_metric: 0.0314\n",
      "Epoch 14/100\n",
      "500/500 [==============================] - 0s 108us/step - loss: 0.0284 - spearman_metric: 0.0176 - val_loss: 0.0554 - val_spearman_metric: 0.0300\n",
      "Epoch 15/100\n",
      "500/500 [==============================] - 0s 108us/step - loss: 0.0268 - spearman_metric: 0.0175 - val_loss: 0.0541 - val_spearman_metric: 0.0287\n",
      "Epoch 16/100\n",
      "500/500 [==============================] - 0s 108us/step - loss: 0.0239 - spearman_metric: 0.0205 - val_loss: 0.0501 - val_spearman_metric: 0.0275\n",
      "Epoch 17/100\n",
      "500/500 [==============================] - 0s 100us/step - loss: 0.0217 - spearman_metric: 0.0185 - val_loss: 0.0470 - val_spearman_metric: 0.0274\n",
      "Epoch 18/100\n",
      "500/500 [==============================] - 0s 94us/step - loss: 0.0200 - spearman_metric: 0.0166 - val_loss: 0.0478 - val_spearman_metric: 0.0266\n",
      "Epoch 19/100\n",
      "500/500 [==============================] - 0s 100us/step - loss: 0.0199 - spearman_metric: 0.0181 - val_loss: 0.0438 - val_spearman_metric: 0.0253\n",
      "Epoch 20/100\n",
      "500/500 [==============================] - 0s 101us/step - loss: 0.0180 - spearman_metric: 0.0150 - val_loss: 0.0476 - val_spearman_metric: 0.0256\n",
      "Epoch 21/100\n",
      "500/500 [==============================] - 0s 94us/step - loss: 0.0185 - spearman_metric: 0.0170 - val_loss: 0.0416 - val_spearman_metric: 0.0248\n",
      "Epoch 22/100\n",
      "500/500 [==============================] - 0s 98us/step - loss: 0.0159 - spearman_metric: 0.0144 - val_loss: 0.0387 - val_spearman_metric: 0.0226\n",
      "Epoch 23/100\n",
      "500/500 [==============================] - 0s 98us/step - loss: 0.0154 - spearman_metric: 0.0150 - val_loss: 0.0397 - val_spearman_metric: 0.0227\n",
      "Epoch 24/100\n",
      "500/500 [==============================] - 0s 96us/step - loss: 0.0147 - spearman_metric: 0.0141 - val_loss: 0.0366 - val_spearman_metric: 0.0221\n",
      "Epoch 25/100\n",
      "500/500 [==============================] - 0s 96us/step - loss: 0.0144 - spearman_metric: 0.0138 - val_loss: 0.0342 - val_spearman_metric: 0.0217\n",
      "Epoch 26/100\n",
      "500/500 [==============================] - 0s 102us/step - loss: 0.0137 - spearman_metric: 0.0118 - val_loss: 0.0328 - val_spearman_metric: 0.0211\n",
      "Epoch 27/100\n",
      "500/500 [==============================] - 0s 96us/step - loss: 0.0132 - spearman_metric: 0.0150 - val_loss: 0.0320 - val_spearman_metric: 0.0209\n",
      "Epoch 28/100\n",
      "500/500 [==============================] - 0s 102us/step - loss: 0.0137 - spearman_metric: 0.0129 - val_loss: 0.0310 - val_spearman_metric: 0.0204\n",
      "Epoch 29/100\n",
      "500/500 [==============================] - 0s 100us/step - loss: 0.0129 - spearman_metric: 0.0117 - val_loss: 0.0296 - val_spearman_metric: 0.0196\n",
      "Epoch 30/100\n",
      "500/500 [==============================] - 0s 99us/step - loss: 0.0122 - spearman_metric: 0.0118 - val_loss: 0.0295 - val_spearman_metric: 0.0198\n",
      "Epoch 31/100\n",
      "500/500 [==============================] - 0s 100us/step - loss: 0.0114 - spearman_metric: 0.0147 - val_loss: 0.0278 - val_spearman_metric: 0.0193\n",
      "Epoch 32/100\n",
      "500/500 [==============================] - 0s 98us/step - loss: 0.0110 - spearman_metric: 0.0127 - val_loss: 0.0278 - val_spearman_metric: 0.0188\n",
      "Epoch 33/100\n",
      "500/500 [==============================] - 0s 106us/step - loss: 0.0107 - spearman_metric: 0.0117 - val_loss: 0.0264 - val_spearman_metric: 0.0181\n",
      "Epoch 34/100\n",
      "500/500 [==============================] - 0s 100us/step - loss: 0.0105 - spearman_metric: 0.0120 - val_loss: 0.0264 - val_spearman_metric: 0.0174\n",
      "Epoch 35/100\n",
      "500/500 [==============================] - 0s 96us/step - loss: 0.0101 - spearman_metric: 0.0107 - val_loss: 0.0247 - val_spearman_metric: 0.0181\n",
      "Epoch 36/100\n",
      "500/500 [==============================] - 0s 94us/step - loss: 0.0097 - spearman_metric: 0.0111 - val_loss: 0.0243 - val_spearman_metric: 0.0174\n",
      "Epoch 37/100\n",
      "500/500 [==============================] - 0s 102us/step - loss: 0.0089 - spearman_metric: 0.0098 - val_loss: 0.0236 - val_spearman_metric: 0.0169\n",
      "Epoch 38/100\n",
      "500/500 [==============================] - 0s 94us/step - loss: 0.0090 - spearman_metric: 0.0096 - val_loss: 0.0227 - val_spearman_metric: 0.0170\n",
      "Epoch 39/100\n",
      "500/500 [==============================] - 0s 90us/step - loss: 0.0087 - spearman_metric: 0.0105 - val_loss: 0.0227 - val_spearman_metric: 0.0165\n",
      "Epoch 40/100\n",
      "500/500 [==============================] - 0s 94us/step - loss: 0.0082 - spearman_metric: 0.0096 - val_loss: 0.0217 - val_spearman_metric: 0.0157\n",
      "Epoch 41/100\n",
      "500/500 [==============================] - 0s 98us/step - loss: 0.0083 - spearman_metric: 0.0107 - val_loss: 0.0217 - val_spearman_metric: 0.0161\n",
      "Epoch 42/100\n",
      "500/500 [==============================] - 0s 94us/step - loss: 0.0081 - spearman_metric: 0.0083 - val_loss: 0.0208 - val_spearman_metric: 0.0154\n",
      "Epoch 43/100\n",
      "500/500 [==============================] - 0s 91us/step - loss: 0.0077 - spearman_metric: 0.0087 - val_loss: 0.0201 - val_spearman_metric: 0.0153\n",
      "Epoch 44/100\n",
      "500/500 [==============================] - 0s 94us/step - loss: 0.0073 - spearman_metric: 0.0090 - val_loss: 0.0197 - val_spearman_metric: 0.0148\n",
      "Epoch 45/100\n",
      "500/500 [==============================] - 0s 91us/step - loss: 0.0070 - spearman_metric: 0.0072 - val_loss: 0.0199 - val_spearman_metric: 0.0144\n",
      "Epoch 46/100\n",
      "500/500 [==============================] - 0s 93us/step - loss: 0.0070 - spearman_metric: 0.0093 - val_loss: 0.0197 - val_spearman_metric: 0.0146\n",
      "Epoch 47/100\n",
      "500/500 [==============================] - 0s 90us/step - loss: 0.0070 - spearman_metric: 0.0089 - val_loss: 0.0199 - val_spearman_metric: 0.0148\n",
      "Epoch 48/100\n",
      "500/500 [==============================] - 0s 94us/step - loss: 0.0072 - spearman_metric: 0.0077 - val_loss: 0.0182 - val_spearman_metric: 0.0142\n",
      "Epoch 49/100\n",
      "500/500 [==============================] - 0s 91us/step - loss: 0.0066 - spearman_metric: 0.0081 - val_loss: 0.0178 - val_spearman_metric: 0.0141\n",
      "Epoch 50/100\n",
      "500/500 [==============================] - 0s 92us/step - loss: 0.0063 - spearman_metric: 0.0076 - val_loss: 0.0179 - val_spearman_metric: 0.0137\n",
      "Epoch 51/100\n",
      "500/500 [==============================] - 0s 90us/step - loss: 0.0062 - spearman_metric: 0.0065 - val_loss: 0.0172 - val_spearman_metric: 0.0140\n",
      "Epoch 52/100\n",
      "500/500 [==============================] - 0s 92us/step - loss: 0.0060 - spearman_metric: 0.0069 - val_loss: 0.0173 - val_spearman_metric: 0.0134\n",
      "Epoch 53/100\n",
      "500/500 [==============================] - 0s 94us/step - loss: 0.0059 - spearman_metric: 0.0078 - val_loss: 0.0163 - val_spearman_metric: 0.0134\n",
      "Epoch 54/100\n",
      "500/500 [==============================] - 0s 93us/step - loss: 0.0059 - spearman_metric: 0.0082 - val_loss: 0.0160 - val_spearman_metric: 0.0127\n",
      "Epoch 55/100\n",
      "500/500 [==============================] - 0s 92us/step - loss: 0.0059 - spearman_metric: 0.0085 - val_loss: 0.0168 - val_spearman_metric: 0.0134\n",
      "Epoch 56/100\n",
      "500/500 [==============================] - 0s 91us/step - loss: 0.0056 - spearman_metric: 0.0062 - val_loss: 0.0154 - val_spearman_metric: 0.0124\n",
      "Epoch 57/100\n",
      "500/500 [==============================] - 0s 90us/step - loss: 0.0055 - spearman_metric: 0.0055 - val_loss: 0.0154 - val_spearman_metric: 0.0122\n",
      "Epoch 58/100\n",
      "500/500 [==============================] - 0s 94us/step - loss: 0.0055 - spearman_metric: 0.0077 - val_loss: 0.0151 - val_spearman_metric: 0.0122\n",
      "Epoch 59/100\n",
      "500/500 [==============================] - 0s 100us/step - loss: 0.0057 - spearman_metric: 0.0077 - val_loss: 0.0148 - val_spearman_metric: 0.0117\n",
      "Epoch 60/100\n",
      "500/500 [==============================] - 0s 94us/step - loss: 0.0053 - spearman_metric: 0.0063 - val_loss: 0.0148 - val_spearman_metric: 0.0118\n",
      "Epoch 61/100\n",
      "500/500 [==============================] - 0s 96us/step - loss: 0.0050 - spearman_metric: 0.0063 - val_loss: 0.0142 - val_spearman_metric: 0.0114\n",
      "Epoch 62/100\n",
      "500/500 [==============================] - 0s 99us/step - loss: 0.0050 - spearman_metric: 0.0052 - val_loss: 0.0148 - val_spearman_metric: 0.0119\n",
      "Epoch 63/100\n",
      "500/500 [==============================] - 0s 102us/step - loss: 0.0053 - spearman_metric: 0.0060 - val_loss: 0.0145 - val_spearman_metric: 0.0115\n",
      "Epoch 64/100\n",
      "500/500 [==============================] - 0s 102us/step - loss: 0.0050 - spearman_metric: 0.0057 - val_loss: 0.0140 - val_spearman_metric: 0.0115\n",
      "Epoch 65/100\n",
      "500/500 [==============================] - 0s 100us/step - loss: 0.0050 - spearman_metric: 0.0053 - val_loss: 0.0138 - val_spearman_metric: 0.0112\n",
      "Epoch 66/100\n",
      "500/500 [==============================] - 0s 102us/step - loss: 0.0046 - spearman_metric: 0.0059 - val_loss: 0.0141 - val_spearman_metric: 0.0111\n",
      "Epoch 67/100\n",
      "500/500 [==============================] - 0s 96us/step - loss: 0.0047 - spearman_metric: 0.0069 - val_loss: 0.0138 - val_spearman_metric: 0.0110\n",
      "Epoch 68/100\n",
      "500/500 [==============================] - 0s 94us/step - loss: 0.0045 - spearman_metric: 0.0060 - val_loss: 0.0129 - val_spearman_metric: 0.0109\n",
      "Epoch 69/100\n",
      "500/500 [==============================] - 0s 90us/step - loss: 0.0047 - spearman_metric: 0.0056 - val_loss: 0.0137 - val_spearman_metric: 0.0108\n",
      "Epoch 70/100\n",
      "500/500 [==============================] - 0s 94us/step - loss: 0.0046 - spearman_metric: 0.0056 - val_loss: 0.0125 - val_spearman_metric: 0.0107\n",
      "Epoch 71/100\n",
      "500/500 [==============================] - 0s 98us/step - loss: 0.0043 - spearman_metric: 0.0065 - val_loss: 0.0125 - val_spearman_metric: 0.0110\n",
      "Epoch 72/100\n",
      "500/500 [==============================] - 0s 110us/step - loss: 0.0042 - spearman_metric: 0.0070 - val_loss: 0.0125 - val_spearman_metric: 0.0106\n",
      "Epoch 73/100\n",
      "500/500 [==============================] - 0s 96us/step - loss: 0.0042 - spearman_metric: 0.0050 - val_loss: 0.0120 - val_spearman_metric: 0.0100\n",
      "Epoch 74/100\n",
      "500/500 [==============================] - 0s 100us/step - loss: 0.0042 - spearman_metric: 0.0050 - val_loss: 0.0120 - val_spearman_metric: 0.0098\n",
      "Epoch 75/100\n",
      "500/500 [==============================] - 0s 96us/step - loss: 0.0042 - spearman_metric: 0.0069 - val_loss: 0.0118 - val_spearman_metric: 0.0107\n",
      "Epoch 76/100\n",
      "500/500 [==============================] - 0s 92us/step - loss: 0.0045 - spearman_metric: 0.0070 - val_loss: 0.0129 - val_spearman_metric: 0.0105\n",
      "Epoch 77/100\n",
      "500/500 [==============================] - 0s 96us/step - loss: 0.0050 - spearman_metric: 0.0053 - val_loss: 0.0130 - val_spearman_metric: 0.0100\n",
      "Epoch 78/100\n",
      "500/500 [==============================] - 0s 94us/step - loss: 0.0041 - spearman_metric: 0.0056 - val_loss: 0.0129 - val_spearman_metric: 0.0097\n",
      "Epoch 79/100\n",
      "500/500 [==============================] - 0s 102us/step - loss: 0.0039 - spearman_metric: 0.0056 - val_loss: 0.0112 - val_spearman_metric: 0.0102\n",
      "Epoch 80/100\n",
      "500/500 [==============================] - 0s 100us/step - loss: 0.0039 - spearman_metric: 0.0049 - val_loss: 0.0111 - val_spearman_metric: 0.0098\n",
      "Epoch 81/100\n",
      "500/500 [==============================] - 0s 102us/step - loss: 0.0038 - spearman_metric: 0.0058 - val_loss: 0.0112 - val_spearman_metric: 0.0097\n",
      "Epoch 82/100\n",
      "500/500 [==============================] - 0s 100us/step - loss: 0.0037 - spearman_metric: 0.0057 - val_loss: 0.0112 - val_spearman_metric: 0.0098\n",
      "Epoch 83/100\n",
      "500/500 [==============================] - 0s 100us/step - loss: 0.0037 - spearman_metric: 0.0056 - val_loss: 0.0108 - val_spearman_metric: 0.0097\n",
      "Epoch 84/100\n",
      "500/500 [==============================] - 0s 100us/step - loss: 0.0036 - spearman_metric: 0.0054 - val_loss: 0.0106 - val_spearman_metric: 0.0094\n",
      "Epoch 85/100\n",
      "500/500 [==============================] - 0s 98us/step - loss: 0.0036 - spearman_metric: 0.0049 - val_loss: 0.0105 - val_spearman_metric: 0.0093\n",
      "Epoch 86/100\n",
      "500/500 [==============================] - 0s 99us/step - loss: 0.0035 - spearman_metric: 0.0046 - val_loss: 0.0106 - val_spearman_metric: 0.0093\n",
      "Epoch 87/100\n",
      "500/500 [==============================] - 0s 93us/step - loss: 0.0038 - spearman_metric: 0.0061 - val_loss: 0.0106 - val_spearman_metric: 0.0091\n",
      "Epoch 88/100\n",
      "500/500 [==============================] - 0s 92us/step - loss: 0.0038 - spearman_metric: 0.0047 - val_loss: 0.0103 - val_spearman_metric: 0.0090\n",
      "Epoch 89/100\n",
      "500/500 [==============================] - 0s 94us/step - loss: 0.0033 - spearman_metric: 0.0057 - val_loss: 0.0104 - val_spearman_metric: 0.0091\n",
      "Epoch 90/100\n",
      "500/500 [==============================] - 0s 94us/step - loss: 0.0034 - spearman_metric: 0.0050 - val_loss: 0.0100 - val_spearman_metric: 0.0091\n",
      "Epoch 91/100\n",
      "500/500 [==============================] - 0s 102us/step - loss: 0.0035 - spearman_metric: 0.0049 - val_loss: 0.0102 - val_spearman_metric: 0.0084\n",
      "Epoch 92/100\n",
      "500/500 [==============================] - 0s 98us/step - loss: 0.0034 - spearman_metric: 0.0043 - val_loss: 0.0101 - val_spearman_metric: 0.0090\n",
      "Epoch 93/100\n",
      "500/500 [==============================] - 0s 94us/step - loss: 0.0033 - spearman_metric: 0.0043 - val_loss: 0.0097 - val_spearman_metric: 0.0085\n",
      "Epoch 94/100\n",
      "500/500 [==============================] - 0s 94us/step - loss: 0.0032 - spearman_metric: 0.0041 - val_loss: 0.0096 - val_spearman_metric: 0.0086\n",
      "Epoch 95/100\n",
      "500/500 [==============================] - 0s 90us/step - loss: 0.0032 - spearman_metric: 0.0051 - val_loss: 0.0095 - val_spearman_metric: 0.0083\n",
      "Epoch 96/100\n",
      "500/500 [==============================] - 0s 92us/step - loss: 0.0032 - spearman_metric: 0.0057 - val_loss: 0.0097 - val_spearman_metric: 0.0085\n",
      "Epoch 97/100\n",
      "500/500 [==============================] - 0s 96us/step - loss: 0.0031 - spearman_metric: 0.0045 - val_loss: 0.0095 - val_spearman_metric: 0.0082\n",
      "Epoch 98/100\n",
      "500/500 [==============================] - 0s 94us/step - loss: 0.0030 - spearman_metric: 0.0042 - val_loss: 0.0093 - val_spearman_metric: 0.0086\n",
      "Epoch 99/100\n",
      "500/500 [==============================] - 0s 94us/step - loss: 0.0033 - spearman_metric: 0.0047 - val_loss: 0.0095 - val_spearman_metric: 0.0080\n",
      "Epoch 100/100\n",
      "500/500 [==============================] - 0s 98us/step - loss: 0.0031 - spearman_metric: 0.0040 - val_loss: 0.0092 - val_spearman_metric: 0.0082\n",
      "500/500 [==============================] - 0s 46us/step\n",
      "500/500 [==============================] - 0s 46us/step\n",
      "Train MSE: 0.003, Test MSE: 0.009\n",
      "Train Corr: 0.005, Test Corr: 0.008\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# mlp with scaled outputs on the regression problem\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=20, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "# compile model using spearman_correlation los function\n",
    "#model.compile(loss=spearman_loss, optimizer=SGD(lr=0.01, momentum=0.9), metrics=[spearman_metric]) #no gradient, cannot use\n",
    "model.compile(loss='mean_squared_error', optimizer=SGD(lr=0.01, momentum=0.9), metrics=[spearman_metric])\n",
    "#model.compile(loss='mean_squared_error', optimizer=SGD(lr=0.01, momentum=0.9))\n",
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=100, verbose=1)\n",
    "# evaluate the model\n",
    "train_e = model.evaluate(trainX, trainy, verbose=1)\n",
    "test_e = model.evaluate(testX, testy, verbose=1)\n",
    "print('Train MSE: %.3f, Test MSE: %.3f' % (train_e[0], test_e[0])) #when using custom loss and custom metric\n",
    "print('Train Corr: %.3f, Test Corr: %.3f' % (train_e[1], test_e[1])) #when using custom loss and custom metric\n",
    "#print('Train MSE: %.3f, Test MSE: %.3f' % (train_e, test_e)) \n",
    "#plot loss during training\n",
    "pyplot.title('Loss / Error')\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.py4u.net/discuss/199027"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 4         \n",
      "=================================================================\n",
      "Total params: 4\n",
      "Trainable params: 4\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "@tf.custom_gradient\n",
    "def py_loss_fn(y_true, y_pred):\n",
    "  \"\"\" This function takes eager tensors as inputs which can be explicitly\n",
    "  converted to np.arrays via EagerTensor.numpy() or implicitly converted\n",
    "  by applying numpy operations to them.\n",
    "\n",
    "  However, once tf operations are no longer used it means that the function has to\n",
    "  implement its own gradient function.\n",
    "  \"\"\"\n",
    "  def grad(dy):\n",
    "    \"\"\" Compute gradients for function inputs.\n",
    "        Ignore input[0] (y_true) since that is model.targets[0]\n",
    "    \"\"\"\n",
    "    g = np.mean(-dy * np.sign(y_true - y_pred), axis=1)[:, np.newaxis]\n",
    "    return None, g\n",
    "\n",
    "  return np.mean(np.abs(y_true - y_pred), axis=1), grad\n",
    "\n",
    "def eager_loss_fn(y_true, y_pred):\n",
    "  \"\"\" If tf operations are used on eager tensors auto diff works without issues\n",
    "  \"\"\"\n",
    "  return tf.reduce_mean(tf.abs(y_true - y_pred))\n",
    "\n",
    "def loss_fn(y_true, y_pred, **kw_args):\n",
    "  \"\"\" This function takes tensors as inputs. Numpy operations are not valid.\n",
    "  \"\"\"\n",
    "#   loss = tf.py_function(eager_loss_fn, inp=[y_true, y_pred], Tout=tf.float32)\n",
    "  \n",
    "  loss = tf.py_function(py_loss_fn, inp=[y_true, y_pred], Tout=tf.float32)\n",
    "\n",
    "  return loss\n",
    "\n",
    "def make_model():\n",
    "  \"\"\" Linear regression model with custom loss \"\"\"\n",
    "  inp = Input(shape=(4,))\n",
    "  out = Dense(1, use_bias=False)(inp)\n",
    "  model = Model(inp, out)\n",
    "  model.compile('adam', loss_fn)\n",
    "  return model\n",
    "\n",
    "model = make_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2551605701446533\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "FACTORS = np.arange(4) + 1\n",
    "def test_fn(x):\n",
    "  return np.dot(x, FACTORS.T)\n",
    "\n",
    "X = np.random.rand(3, 4)\n",
    "Y = np.apply_along_axis(test_fn, 1, X)\n",
    "\n",
    "history = model.fit(X, Y, epochs=1000, verbose=False)\n",
    "print(history.history['loss'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/aponte411/numerai_train/blob/master/metrics.py\n",
    "https://pretagteam.com/question/how-to-compute-spearman-correlation-in-tensorflow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
