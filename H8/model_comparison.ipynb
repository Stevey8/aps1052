{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "176549a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fill in the missing code. The lines with missing code have the string \"#####\" or '*'\n",
    "\"INSTRUCTIONS\" comments explain how to fill in the mising code.\n",
    "the outputfile.txt has the printouts from the program.\n",
    "Your results should be similar within reason, if not, re-run the program,\n",
    "since we are using RandomizedSearchCV, meaning there is some randomness involved.\n",
    "Actually, we added np.random.seed() to fix the results, so you can check them.\n",
    "\n",
    "You will be filling in code in two types of models:\n",
    "1. a regression model and\n",
    "2. a classification model.\n",
    "\n",
    "Most of the time, because of similarities,\n",
    "you can cut and paste from one model to the other.\n",
    "But in a few instances, you cannot do this, so\n",
    "you need to pay attention.\n",
    "Also, in some cases,\n",
    "you will find a \"hint\" for a solution \n",
    "in one of the two scripts (regression or classification)\n",
    "that you can use as inspiration for the other.\n",
    "\n",
    "This double task gives you the opportunity to look at the results\n",
    "in both regression and classification approaches.\n",
    "\n",
    "At the bottom, you will find some questions that we pose.\n",
    "You do not need to write and turn in the answer to these questions,\n",
    "but we strongly recommend you find out the answers to them.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "In this script you will learn how to use pipelines to compare models\n",
    "so as to select a better  model from a list of candidates.\n",
    "\n",
    "There are a number of open source libraries in python that\n",
    "allow you to compare models and are very easy to use:\n",
    "\n",
    "1. PyCaret's compare_models() function allows you to compare Scikit-Learn models.\n",
    "You can read about here:\n",
    "https://archive.ph/vSeYy\n",
    "https://archive.ph/h5HI3\n",
    "https://pycaret.readthedocs.io/en/latest/index.html\n",
    "\n",
    "2. H2O also has an AutoML function that allows you to compare models.\n",
    "https://archive.ph/8DdJ4\n",
    "\n",
    "3. Microsoft also has an AutoML function:\n",
    "https://azure.microsoft.com/en-us/services/machine-learning/automatedml/\n",
    "https://archive.ph/GuH96\n",
    "\n",
    "\n",
    "However, it is important for you to know what these libraries are doing under wraps,\n",
    "and it is always good to have a customized way to compare models that you can modify to your liking.\n",
    "This is why you will compare models using the scikit-learn pipeline in this homework.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import fAux\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "from sklearn import preprocessing\n",
    "\n",
    "np.random.seed(1) #to fix the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c90fa412",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('EURUSD_H3_200001030000_202107201800.csv', sep='\\t')\n",
    "#df = pd.read_csv('GBPUSD_H3_200001030000_202107201800.csv', sep='\\t')\n",
    "#df = pd.read_csv('NZDUSD_H3_200001030000_202107201800.csv', sep='\\t')\n",
    "df = pd.read_csv('USDCAD_H3_200001030000_202107201800.csv', sep='\\t')\n",
    "#df = pd.read_csv('USDCHF_H3_200001030000_202107201800.csv', sep='\\t')\n",
    "\n",
    "df['<DATETIME>'] = pd.to_datetime(df['<DATE>'] + ' ' + df['<TIME>'])\n",
    "df = df.set_index('<DATETIME>')\n",
    "df.drop(['<TIME>'], axis=1, inplace=True)\n",
    "df.drop(['<DATE>'], axis=1, inplace=True)\n",
    "\n",
    "#buld the best window features after the exploratory data analysis:\n",
    "for n in [1,2,3,4,11,14]:\n",
    "    name = 'ret' + str(n)\n",
    "    df[name] = df[\"<OPEN>\"].pct_change(periods=n) #for trading with open\n",
    "    #df[name] = df[\"<CLOSE>\"].pct_change(periods=n) #for trading with close\n",
    "\n",
    "#build date-time features\n",
    "df[\"hour\"] = df.index.hour.values\n",
    "df[\"day\"] = df.index.dayofweek.values\n",
    "\n",
    "#build target assuming we know today's open\n",
    "df['retFut1'] = df['<OPEN>'].pct_change(1).shift(-1).fillna(0) #if you enter the trade immediately after the open\n",
    "#df['retFut1'] = df['<CLOSE>'].pct_change(1).shift(-1) #if you wait until the close to enter the trade\n",
    "#df = np.log(df+1)\n",
    "\n",
    "#transform the target\n",
    "df['retFut1_categ'] = np.where((df['retFut1'] > 0), 1, 0)\n",
    "\n",
    "#Since we are trading right after the open, \n",
    "#we only know yesterday's  high low close volume spread etc.\n",
    "df['<HIGH>'] = df['<HIGH>'].shift(1)\n",
    "df['<LOW>'] = df['<LOW>'].shift(1)\n",
    "df['<CLOSE>'] = df['<CLOSE>'].shift(1)\n",
    "df['<VOL>'] = df['<VOL>'].shift(1)\n",
    "df['<SPREAD>'] = df['<SPREAD>'].shift(1)\n",
    "\n",
    "#select the features (by dropping)\n",
    "cols_to_drop = [\"<OPEN>\",\"<HIGH>\",\"<LOW>\",\"<CLOSE>\",\"<TICKVOL>\",\"<VOL>\",\"<SPREAD>\"]  #optional\n",
    "df_filtered = df.drop(cols_to_drop, axis=1)\n",
    "\n",
    "#distribute the df data into X inputs and y target\n",
    "X = df_filtered.drop(['retFut1', 'retFut1_categ'], axis=1) \n",
    "y = df_filtered[['retFut1_categ']]\n",
    "\n",
    "\n",
    "#MixedNB expects categorical features to be label encoded\n",
    "#as per  https://archive.ph/Ki1DS#selection-5521.0-5521.12\n",
    "le = preprocessing.LabelEncoder()\n",
    "X.hour = le.fit_transform(X.hour)\n",
    "X.day = le.fit_transform(X.day)\n",
    "\n",
    "#select the samples\n",
    "x_train = X.iloc[0:10000]\n",
    "x_test = X.iloc[10000:12000]\n",
    "\n",
    "y_train = y.iloc[0:10000]\n",
    "y_test = y.iloc[10000:12000]\n",
    "\n",
    "df_train = df_filtered.iloc[0:10000]\n",
    "df_test = df_filtered.iloc[10000:12000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c37da637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ret1     float64\n",
      "ret2     float64\n",
      "ret3     float64\n",
      "ret4     float64\n",
      "ret11    float64\n",
      "ret14    float64\n",
      "hour       int64\n",
      "day        int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#set up the grid search and fit\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import make_scorer \n",
    "from sklearn import preprocessing\n",
    "import phik\n",
    "from phik.report import plot_correlation_matrix\n",
    "from scipy.special import ndtr\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "def phi_k(y_true, y_pred):\n",
    "    dfc = pd.DataFrame({'y_true': y_true, 'y_pred': y_pred})\n",
    "    try:\n",
    "        phi_k_corr = dfc.phik_matrix(interval_cols=[]).iloc[1,0]\n",
    "        phi_k_sig  = dfc.significance_matrix(interval_cols=[]).iloc[1,0]\n",
    "        phi_k_p_val = 1 - ndtr(phi_k_sig) \n",
    "    except:\n",
    "        phi_k_corr = 0\n",
    "        phi_k_p_val = 0\n",
    "    #print(phi_k_corr)\n",
    "    print(phi_k_p_val)\n",
    "    return phi_k_corr\n",
    "\n",
    "\n",
    "#myscorer = None #use default accuracy score\n",
    "myscorer = make_scorer(phi_k, greater_is_better=True)\n",
    "\n",
    "numeric_sub_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value=0)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=0.95))])\n",
    "categorical_sub_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value=0)),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "    \n",
    "print(x_train.dtypes)\n",
    "numeric_features_ix = x_train.select_dtypes(include=['float64']).columns\n",
    "categorical_features_ix = x_train.select_dtypes(include=['int64']).columns\n",
    "\n",
    "#Note: transformer 3-element tuples can be: ('name', function or pipeline, column_number_list or column_index)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_sub_pipeline, numeric_features_ix),\n",
    "        ('cat', categorical_sub_pipeline, categorical_features_ix)], remainder='passthrough')\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from mixed_naive_bayes import MixedNB\n",
    "\n",
    "rfc_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', RandomForestClassifier())])\n",
    "rfc_pipeline.fit(x_train, y_train.values.squeeze())\n",
    "y_pred = rfc_pipeline.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a962d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "INSTRUCTIONS\n",
    "Add a few more classifiers to this list\n",
    "You may want to compare especially random forest with mixed naive bayes\n",
    "however remember that MixedNB needs to know the categorical features column indexes, so\n",
    "you need to use inspect_me to find out which catagorical features column indexes to include\n",
    "after the preprocessor has finished preprocessing the inputs.\n",
    "If you add MixedNB, \n",
    "Add PCA() to the numeric_sub_pipeline since the inputs of MixedNB are assumed to be non-correlated (=\"naive\")\n",
    "LabelEncode the categorical features as per the tutorial here: https://archive.ph/Ki1DS#selection-5521.0-5521.12\n",
    "This involves some thinking, because unlike PCA(), LabelEncode() does not take as input a matrix X but an array y.\n",
    "see: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n",
    "\"\"\"\n",
    "\n",
    "inspect_me = preprocessor.fit_transform(x_train) #columns 6 to 18 are categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e76af7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ret1</th>\n",
       "      <th>ret2</th>\n",
       "      <th>ret3</th>\n",
       "      <th>ret4</th>\n",
       "      <th>ret11</th>\n",
       "      <th>ret14</th>\n",
       "      <th>hour</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;DATETIME&gt;</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-01-03 00:00:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-03 03:00:00</th>\n",
       "      <td>-0.001245</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-03 06:00:00</th>\n",
       "      <td>0.000485</td>\n",
       "      <td>-0.000761</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-03 09:00:00</th>\n",
       "      <td>0.001038</td>\n",
       "      <td>0.001523</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-03 12:00:00</th>\n",
       "      <td>-0.001590</td>\n",
       "      <td>-0.000554</td>\n",
       "      <td>-0.000069</td>\n",
       "      <td>-0.001314</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         ret1      ret2      ret3      ret4  ret11  ret14  \\\n",
       "<DATETIME>                                                                  \n",
       "2000-01-03 00:00:00       NaN       NaN       NaN       NaN    NaN    NaN   \n",
       "2000-01-03 03:00:00 -0.001245       NaN       NaN       NaN    NaN    NaN   \n",
       "2000-01-03 06:00:00  0.000485 -0.000761       NaN       NaN    NaN    NaN   \n",
       "2000-01-03 09:00:00  0.001038  0.001523  0.000277       NaN    NaN    NaN   \n",
       "2000-01-03 12:00:00 -0.001590 -0.000554 -0.000069 -0.001314    NaN    NaN   \n",
       "\n",
       "                     hour  day  \n",
       "<DATETIME>                      \n",
       "2000-01-03 00:00:00     0    0  \n",
       "2000-01-03 03:00:00     1    0  \n",
       "2000-01-03 06:00:00     2    0  \n",
       "2000-01-03 09:00:00     3    0  \n",
       "2000-01-03 12:00:00     4    0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef9bedf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ret1    -0.000471\n",
      "ret2    -0.000538\n",
      "ret3     0.000404\n",
      "ret4    -0.000336\n",
      "ret11    0.004801\n",
      "ret14    0.003037\n",
      "hour     3.000000\n",
      "day      2.000000\n",
      "Name: 2000-05-03 09:00:00, dtype: float64\n",
      "ret1     0.000606\n",
      "ret2     0.000135\n",
      "ret3     0.000067\n",
      "ret4     0.001010\n",
      "ret11    0.005409\n",
      "ret14    0.005954\n",
      "hour     4.000000\n",
      "day      2.000000\n",
      "Name: 2000-05-03 12:00:00, dtype: float64\n",
      "ret1     0.001547\n",
      "ret2     0.002153\n",
      "ret3     0.001682\n",
      "ret4     0.001614\n",
      "ret11    0.007646\n",
      "ret14    0.006692\n",
      "hour     5.000000\n",
      "day      2.000000\n",
      "Name: 2000-05-03 15:00:00, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(x_train.iloc[699])\n",
    "print(x_train.iloc[700])\n",
    "print(x_train.iloc[701])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87b1339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 19)\n",
      "[-0.28494324 -0.22763679  0.16098171 -0.09083631  0.97695678  0.57274504\n",
      "  0.          0.          0.          1.          0.          0.\n",
      "  0.          0.          0.          0.          1.          0.\n",
      "  0.        ]\n",
      "[0.38445219 0.07091884 0.03838638 0.33747749 1.09727532 1.09177412\n",
      " 0.         0.         0.         0.         1.         0.\n",
      " 0.         0.         0.         0.         1.         0.\n",
      " 0.        ]\n",
      "[0.96971132 0.967084   0.62622034 0.5297703  1.53946549 1.22326384\n",
      " 0.         0.         0.         0.         0.         1.\n",
      " 0.         0.         0.         0.         1.         0.\n",
      " 0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(inspect_me.shape)\n",
    "print(inspect_me[699])\n",
    "print(inspect_me[700])\n",
    "print(inspect_me[701])\n",
    "\n",
    "# one hot encoded; col 6-18 are categorical "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac80e141",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aa99af94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(n_neighbors=3)\n",
      "model score: 0.510\n",
      "SVC(C=0.025, probability=True)\n",
      "model score: 0.526\n",
      "NuSVC(probability=True)\n",
      "model score: 0.510\n",
      "RandomForestClassifier(random_state=42)\n",
      "model score: 0.535\n",
      "LogisticRegression(max_iter=1000, random_state=42)\n",
      "model score: 0.559\n"
     ]
    }
   ],
   "source": [
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"rbf\", C=0.025, probability=True),\n",
    "    NuSVC(probability=True),\n",
    "    RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    LogisticRegression(max_iter=1000, random_state=42),\n",
    "    # MixedNB(categorical_features=np.arange(5, 18))  # columns 6 to 18 are categorical\n",
    "    ]\n",
    "for classifier in classifiers:\n",
    "    classifier_pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', classifier)])\n",
    "    classifier_pipe.fit(x_train, y_train.values.squeeze())   \n",
    "    print(classifier)\n",
    "    print(\"model score: %.3f\" % classifier_pipe.score(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c028bb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpsc330",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
